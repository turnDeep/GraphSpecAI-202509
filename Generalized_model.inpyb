import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch_geometric.nn import GATv2Conv, GCNConv, GlobalAttention, global_mean_pool, global_add_pool, global_max_pool
from torch_geometric.data import Data, Batch
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import AllChem, Descriptors, MACCSkeys
from tqdm import tqdm
import logging
import copy
import random
import math
from torch.optim.lr_scheduler import OneCycleLR, LambdaLR

# ロガーの設定
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# パス設定
DATA_PATH = "data/"
MOL_FILES_PATH = os.path.join(DATA_PATH, "mol_files/")
MSP_FILE_PATH = os.path.join(DATA_PATH, "NIST17.MSP")

# 原子の特徴マッピング（拡張版）
ATOM_FEATURES = {
    'C': 0, 'N': 1, 'O': 2, 'S': 3, 'F': 4, 'Cl': 5, 'Br': 6, 'I': 7, 'P': 8,
    'Si': 9, 'B': 10, 'Na': 11, 'K': 12, 'Li': 13, 'Mg': 14, 'Ca': 15, 'Fe': 16,
    'Co': 17, 'Ni': 18, 'Cu': 19, 'Zn': 20, 'H': 21, 'OTHER': 22
}

# 結合の特徴マッピング
BOND_FEATURES = {
    Chem.rdchem.BondType.SINGLE: 0,
    Chem.rdchem.BondType.DOUBLE: 1,
    Chem.rdchem.BondType.TRIPLE: 2,
    Chem.rdchem.BondType.AROMATIC: 3
}

# 最大m/z値の設定 - エラー修正: 大きな値をカバーするために増加
MAX_MZ = 2000

# フラグメントパターンの数（MACCSキー使用）
NUM_FRAGS = 167  # MACCSキーのビット数

# 特徴的なm/z値のリスト (よく現れるフラグメントイオンに対応)
IMPORTANT_MZ = [18, 28, 43, 57, 71, 73, 77, 91, 105, 115, 128, 152, 165, 178, 207]

class CosineSimilarityLoss(nn.Module):
    """重み付きコサイン類似度損失関数"""
    def __init__(self, eps=1e-8, important_mz=None, important_weight=3.0):
        super(CosineSimilarityLoss, self).__init__()
        self.eps = eps
        self.important_mz = important_mz if important_mz is not None else []
        self.important_weight = important_weight
        
    def forward(self, x, y):
        # 正規化
        x_norm = F.normalize(x, p=2, dim=1)
        y_norm = F.normalize(y, p=2, dim=1)
        
        # 特徴的なm/zの重み付け
        if self.important_mz:
            batch_size = x.size(0)
            weights = torch.ones_like(x)
            for mz in self.important_mz:
                if mz < x.size(1):
                    weights[:, mz] = self.important_weight
            
            # 重み付きベクトルで正規化
            x_weighted = x * weights
            y_weighted = y * weights
            
            x_norm = F.normalize(x_weighted, p=2, dim=1)
            y_norm = F.normalize(y_weighted, p=2, dim=1)
        
        # コサイン類似度（-1〜1の範囲）
        cosine = torch.sum(x_norm * y_norm, dim=1)
        
        # 損失を1 - cosineにして、0〜2の範囲に
        loss = 1.0 - cosine
        
        return loss.mean()

class FragmentPredictionLoss(nn.Module):
    """フラグメントパターン予測のための損失関数"""
    def __init__(self):
        super(FragmentPredictionLoss, self).__init__()
        self.bce_loss = nn.BCEWithLogitsLoss()
        
    def forward(self, pred_fragments, true_fragments):
        return self.bce_loss(pred_fragments, true_fragments)

class MoleculeGraphDataset(Dataset):
    def __init__(self, mol_ids, mol_files_path, msp_data, augment=False):
        self.mol_ids = mol_ids
        self.mol_files_path = mol_files_path
        self.msp_data = msp_data
        self.augment = augment
        self.valid_mol_ids = []  # 有効な分子IDを保存するリスト
        self.fragment_patterns = {}  # 分子IDごとのフラグメントパターン
        
        # 前処理で有効な分子IDを抽出
        self._preprocess_mol_ids()
        
    def _preprocess_mol_ids(self):
        """有効な分子IDのみを抽出する"""
        valid_ids = []
        fragment_patterns = {}
        
        for mol_id in tqdm(self.mol_ids, desc="Validating molecules"):
            mol_file = os.path.join(self.mol_files_path, f"ID{mol_id}.MOL")
            try:
                # 分子ファイルが読み込めるか確認
                mol = Chem.MolFromMolFile(mol_file, sanitize=False)
                if mol is None:
                    continue
                
                # 分子の基本的なサニタイズを試みる
                try:
                    # プロパティキャッシュを更新
                    for atom in mol.GetAtoms():
                        atom.UpdatePropertyCache(strict=False)
                    
                    # 部分的なサニタイズ
                    Chem.SanitizeMol(mol, 
                                   sanitizeOps=Chem.SanitizeFlags.SANITIZE_FINDRADICALS|
                                              Chem.SanitizeFlags.SANITIZE_KEKULIZE|
                                              Chem.SanitizeFlags.SANITIZE_SETAROMATICITY|
                                              Chem.SanitizeFlags.SANITIZE_SETCONJUGATION|
                                              Chem.SanitizeFlags.SANITIZE_SETHYBRIDIZATION|
                                              Chem.SanitizeFlags.SANITIZE_SYMMRINGS,
                                   catchErrors=True)
                except Exception as e:
                    logger.warning(f"Skipping molecule ID{mol_id} due to sanitization error: {str(e)}")
                    continue
                
                # グラフに変換できるか確認
                try:
                    _ = self._mol_to_graph(mol_file)
                    valid_ids.append(mol_id)
                    
                    # フラグメントパターンを生成（マルチタスク学習用）
                    try:
                        # MACCSフィンガープリントを計算
                        fragments = self._generate_fragment_features(mol)
                        fragment_patterns[mol_id] = fragments
                    except Exception as e:
                        logger.warning(f"Could not generate fragments for ID{mol_id}: {str(e)}")
                        # フラグメント計算に失敗した場合は0ベクトルを使用
                        fragment_patterns[mol_id] = np.zeros(NUM_FRAGS)
                        
                except Exception as e:
                    logger.warning(f"Skipping molecule ID{mol_id} due to graph conversion error: {str(e)}")
                    continue
                    
            except Exception as e:
                logger.warning(f"Skipping molecule ID{mol_id} due to error: {str(e)}")
                continue
                
        self.valid_mol_ids = valid_ids
        self.fragment_patterns = fragment_patterns
        logger.info(f"Found {len(valid_ids)} valid molecules out of {len(self.mol_ids)}")
        
    def _generate_fragment_features(self, mol):
        """分子のフラグメント特徴量を生成"""
        # MACCSフィンガープリントを計算
        maccs = MACCSkeys.GenMACCSKeys(mol)
        maccs_bits = np.zeros(NUM_FRAGS)
        
        # ビットを取得
        for i in range(NUM_FRAGS):
            if maccs.GetBit(i):
                maccs_bits[i] = 1.0
                
        return maccs_bits
        
    def __len__(self):
        return len(self.valid_mol_ids)
    
    def __getitem__(self, idx):
        mol_id = self.valid_mol_ids[idx]
        mol_file = os.path.join(self.mol_files_path, f"ID{mol_id}.MOL")
        
        # MOLファイルからグラフ表現を生成
        graph_data = self._mol_to_graph(mol_file)
        
        # MSPデータからマススペクトルを取得
        mass_spectrum = self.msp_data.get(mol_id, np.zeros(MAX_MZ))
        mass_spectrum = self._preprocess_spectrum(mass_spectrum)
        
        # フラグメントパターンを取得
        fragment_pattern = self.fragment_patterns.get(mol_id, np.zeros(NUM_FRAGS))
        
        return graph_data, torch.FloatTensor(mass_spectrum), torch.FloatTensor(fragment_pattern)
    
    def _preprocess_spectrum(self, spectrum):
        """スペクトルの前処理を強化"""
        # ピークの強調 (非ゼロ値のみを考慮)
        non_zero_indices = np.nonzero(spectrum)[0]
        if len(non_zero_indices) > 0:
            # 上位30%のピークを強調
            threshold = np.percentile(spectrum[non_zero_indices], 70)
            spectrum_enhanced = spectrum.copy()
            spectrum_enhanced[spectrum > threshold] *= 1.2
            
            # 特徴的なm/z値のピークを強調
            for mz in IMPORTANT_MZ:
                if mz < len(spectrum_enhanced) and spectrum_enhanced[mz] > 0:
                    spectrum_enhanced[mz] *= 1.5
            
            # 正規化
            if np.max(spectrum_enhanced) > 0:
                spectrum_enhanced = spectrum_enhanced / np.max(spectrum_enhanced) * 100
            
            # スムージング
            smoothed = np.zeros_like(spectrum_enhanced)
            for i in range(len(spectrum_enhanced)):
                window_start = max(0, i - 1)
                window_end = min(len(spectrum_enhanced), i + 2)
                smoothed[i] = np.mean(spectrum_enhanced[window_start:window_end])
            
            # ノイズ除去（小さなピークを除去）
            noise_threshold = 1.0
            smoothed[smoothed < noise_threshold] = 0.0
            
            return smoothed
            
        return spectrum
        
    def _mol_to_graph(self, mol_file):
        """分子をグラフに変換（拡張特徴量あり、エラー処理強化）"""
        # RDKitでMOLファイルを読み込む (サニタイズを無効にして読み込む)
        mol = Chem.MolFromMolFile(mol_file, sanitize=False)
        if mol is None:
            raise ValueError(f"Could not read molecule from {mol_file}")
        
        # 拡張機能: 特徴量の追加
        try:
            # プロパティキャッシュを更新して暗黙的な原子価を計算
            for atom in mol.GetAtoms():
                atom.UpdatePropertyCache(strict=False)
            
            # 部分的なサニタイズ（エラーが発生しやすい操作を除外）
            Chem.SanitizeMol(mol, 
                           sanitizeOps=Chem.SanitizeFlags.SANITIZE_FINDRADICALS|
                                      Chem.SanitizeFlags.SANITIZE_KEKULIZE|
                                      Chem.SanitizeFlags.SANITIZE_SETAROMATICITY|
                                      Chem.SanitizeFlags.SANITIZE_SETCONJUGATION|
                                      Chem.SanitizeFlags.SANITIZE_SETHYBRIDIZATION|
                                      Chem.SanitizeFlags.SANITIZE_SYMMRINGS,
                           catchErrors=True)
            
            # 明示的な水素を追加（安全モード）
            try:
                mol = Chem.AddHs(mol)
            except:
                logger.warning(f"Could not add hydrogens to molecule {mol_file}, proceeding without them")
        except Exception as e:
            logger.warning(f"Error during molecule sanitization {mol_file}: {str(e)}")
            # それでも処理を続行
        
        # 原子情報を取得（拡張特徴量）
        num_atoms = mol.GetNumAtoms()
        x = []
        
        # 特殊な環境の原子を特定（マルチタスク学習用）
        # 修正: GetSSSR()ではなくGetRingInfo().AtomRings()を使用
        ring_info = mol.GetRingInfo().AtomRings()
        aromatic_rings = 0
        for ring in ring_info:
            if all(mol.GetAtomWithIdx(idx).GetIsAromatic() for idx in ring):
                aromatic_rings += 1
                
        for atom in mol.GetAtoms():
            atom_symbol = atom.GetSymbol()
            atom_feature_idx = ATOM_FEATURES.get(atom_symbol, ATOM_FEATURES['OTHER'])
            
            # 基本的な原子タイプの特徴
            atom_feature = [0] * len(ATOM_FEATURES)
            atom_feature[atom_feature_idx] = 1
            
            # 修正: 安全なメソッド呼び出し
            try:
                degree = atom.GetDegree() / 8.0
            except:
                degree = 0.0
                
            try:
                formal_charge = atom.GetFormalCharge() / 8.0
            except:
                formal_charge = 0.0
                
            try:
                radical_electrons = atom.GetNumRadicalElectrons() / 4.0
            except:
                radical_electrons = 0.0
                
            try:
                is_aromatic = atom.GetIsAromatic() * 1.0
            except:
                is_aromatic = 0.0
                
            try:
                atom_mass = atom.GetMass() / 200.0
            except:
                atom_mass = 0.0
                
            try:
                is_in_ring = atom.IsInRing() * 1.0
            except:
                is_in_ring = 0.0
                
            try:
                hybridization = int(atom.GetHybridization()) / 8.0
            except:
                hybridization = 0.0
                
            try:
                explicit_valence = atom.GetExplicitValence() / 8.0
            except:
                explicit_valence = 0.0
                
            try:
                implicit_valence = atom.GetImplicitValence() / 8.0
            except:
                implicit_valence = 0.0
                
            # 追加の環境特徴量
            try:
                is_in_aromatic_ring = (atom.GetIsAromatic() and atom.IsInRing()) * 1.0
            except:
                is_in_aromatic_ring = 0.0
                
            try:
                ring_size = 0
                for ring in ring_info:
                    if atom.GetIdx() in ring:
                        ring_size = max(ring_size, len(ring))
                ring_size = ring_size / 8.0
            except:
                ring_size = 0.0
                
            try:
                num_h = atom.GetTotalNumHs() / 8.0
            except:
                num_h = 0.0
                
            try:
                electronegativity = 0.0
                if atom_symbol in ['O', 'N', 'F', 'Cl', 'Br', 'I']:
                    electronegativity = 1.0
            except:
                electronegativity = 0.0
            
            # ファンシー特徴量（質量分析に関連する特徴）
            try:
                # 典型的なフラグメンテーションサイト
                is_carbonyl_carbon = 0.0
                is_carbonyl_oxygen = 0.0
                neighbors = [n.GetSymbol() for n in atom.GetNeighbors()]
                
                if atom_symbol == 'C' and 'O' in neighbors:
                    for n in atom.GetNeighbors():
                        if n.GetSymbol() == 'O' and n.GetTotalNumHs() == 0:
                            is_carbonyl_carbon = 1.0
                            
                if atom_symbol == 'O' and 'C' in neighbors and atom.GetTotalNumHs() == 0:
                    is_carbonyl_oxygen = 1.0
            except:
                is_carbonyl_carbon = 0.0
                is_carbonyl_oxygen = 0.0
            
            # すべての特徴を結合（基本+拡張特徴量）
            additional_features = [
                degree,                # 結合次数（正規化）
                formal_charge,         # 形式電荷（正規化）
                radical_electrons,     # ラジカル電子数
                is_aromatic,           # 芳香族性
                atom_mass,             # 原子量（正規化）
                is_in_ring,            # 環の一部かどうか
                hybridization,         # 混成軌道
                explicit_valence,      # 原子価
                implicit_valence,      # 暗黙の原子価
                is_in_aromatic_ring,   # 芳香環の一部か
                ring_size,             # 含まれる環のサイズ
                num_h,                 # 水素原子数
                electronegativity,     # 電気陰性度（簡易）
                is_carbonyl_carbon,    # カルボニル炭素か
                is_carbonyl_oxygen,    # カルボニル酸素か
            ]
            
            # すべての特徴を結合
            atom_feature.extend(additional_features)
            x.append(atom_feature)
        
        # 結合情報を取得（拡張特徴量）
        edge_indices = []
        edge_attrs = []
        for bond in mol.GetBonds():
            try:
                i = bond.GetBeginAtomIdx()
                j = bond.GetEndAtomIdx()
                
                # 結合タイプ（デフォルトはSINGLE）
                try:
                    bond_type = BOND_FEATURES.get(bond.GetBondType(), BOND_FEATURES[Chem.rdchem.BondType.SINGLE])
                except:
                    bond_type = BOND_FEATURES[Chem.rdchem.BondType.SINGLE]
                
                # 双方向のエッジを追加
                edge_indices.append([i, j])
                edge_indices.append([j, i])
                
                # 拡張ボンド特徴量
                bond_feature = [0] * len(BOND_FEATURES)
                bond_feature[bond_type] = 1
                
                # 安全な追加ボンド特徴量の取得
                try:
                    is_in_ring = bond.IsInRing() * 1.0
                except:
                    is_in_ring = 0.0
                    
                try:
                    is_conjugated = bond.GetIsConjugated() * 1.0
                except:
                    is_conjugated = 0.0
                    
                try:
                    is_aromatic = bond.GetIsAromatic() * 1.0
                except:
                    is_aromatic = 0.0
                    
                try:
                    stereo = int(bond.GetStereo()) / 8.0
                except:
                    stereo = 0.0
                    
                # フラグメントに関連する特徴
                try:
                    is_rotatable = (not bond.IsInRing()) * 1.0
                except:
                    is_rotatable = 0.0
                    
                try:
                    bond_length = 0.5  # デフォルト値
                    try:
                        # 3D座標がある場合、ボンド長を計算
                        conf = mol.GetConformer()
                        pos1 = conf.GetAtomPosition(i)
                        pos2 = conf.GetAtomPosition(j)
                        bond_length = ((pos1.x - pos2.x)**2 + 
                                      (pos1.y - pos2.y)**2 + 
                                      (pos1.z - pos2.z)**2) ** 0.5 / 5.0  # 正規化
                    except:
                        pass
                except:
                    bond_length = 0.5
                
                # 追加ボンド特徴量
                additional_bond_features = [
                    is_in_ring,          # 環の一部かどうか
                    is_conjugated,       # 共役かどうか
                    is_aromatic,         # 芳香族かどうか
                    stereo,              # 立体化学
                    is_rotatable,        # 回転可能な結合か
                    bond_length,         # 結合長（正規化）
                ]
                
                bond_feature.extend(additional_bond_features)
                edge_attrs.append(bond_feature)
                edge_attrs.append(bond_feature)  # 双方向なので同じ属性
            except Exception as e:
                logger.warning(f"Error processing bond in {mol_file}: {str(e)}")
                continue
        
        # 分子全体の特徴量を計算（安全に）
        mol_features = [0.0] * 16  # デフォルト値で初期化
        
        try:
            mol_features[0] = Descriptors.MolWt(mol) / 1000.0  # 分子量
        except:
            pass
            
        try:
            mol_features[1] = Descriptors.MolLogP(mol) / 10.0  # LogP
        except:
            pass
            
        try:
            mol_features[2] = Descriptors.NumHAcceptors(mol) / 20.0  # 水素結合アクセプター数
        except:
            pass
            
        try:
            mol_features[3] = Descriptors.NumHDonors(mol) / 10.0  # 水素結合ドナー数
        except:
            pass
            
        try:
            mol_features[4] = Descriptors.TPSA(mol) / 200.0  # トポロジカル極性表面積
        except:
            pass
            
        try:
            mol_features[5] = mol.GetNumAtoms() / 100.0  # 原子数
        except:
            pass
            
        try:
            mol_features[6] = Descriptors.NumRotatableBonds(mol) / 20.0  # 回転可能な結合の数
        except:
            pass
            
        try:
            mol_features[7] = Descriptors.NumAromaticRings(mol) / 5.0  # 芳香環の数
        except:
            pass
            
        try:
            mol_features[8] = Descriptors.FractionCSP3(mol)  # sp3炭素の割合
        except:
            pass
            
        try:
            mol_features[9] = Descriptors.NumAliphaticRings(mol) / 5.0  # 脂肪環の数
        except:
            pass
            
        try:
            mol_features[10] = Descriptors.NumAliphaticHeterocycles(mol) / 5.0  # 脂肪族ヘテロ環の数
        except:
            pass
            
        try:
            mol_features[11] = Descriptors.NumAromaticHeterocycles(mol) / 5.0  # 芳香族ヘテロ環の数
        except:
            pass
            
        try:
            mol_features[12] = Descriptors.NumSaturatedRings(mol) / 5.0  # 飽和環の数
        except:
            pass
            
        try:
            mol_features[13] = Descriptors.NumHeteroatoms(mol) / 20.0  # ヘテロ原子の数
        except:
            pass
            
        try:
            mol_features[14] = Descriptors.RingCount(mol) / 10.0  # 環の総数
        except:
            pass
            
        try:
            # 質量分析に関連する特徴量
            carbonyl_count = 0
            for atom in mol.GetAtoms():
                if atom.GetSymbol() == 'C':
                    for neighbor in atom.GetNeighbors():
                        if neighbor.GetSymbol() == 'O' and neighbor.GetTotalNumHs() == 0:
                            carbonyl_count += 1
                            break
            mol_features[15] = carbonyl_count / 10.0  # カルボニル基の数
        except:
            pass
        
        # エッジが存在するか確認
        if not edge_indices:
            # 単一原子分子の場合や結合情報が取得できない場合、セルフループを追加
            for i in range(num_atoms):
                edge_indices.append([i, i])
                
                bond_feature = [0] * len(BOND_FEATURES)
                bond_feature[BOND_FEATURES[Chem.rdchem.BondType.SINGLE]] = 1
                
                # ダミーの追加特徴量
                additional_bond_features = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5]
                bond_feature.extend(additional_bond_features)
                edge_attrs.append(bond_feature)
        
        # PyTorch Geometricのデータ形式に変換
        x = torch.FloatTensor(x)
        edge_index = torch.LongTensor(edge_indices).t().contiguous()
        edge_attr = torch.FloatTensor(edge_attrs)
        global_attr = torch.FloatTensor(mol_features)
        
        # データ拡張（トレーニング時のみ）
        if self.augment and np.random.random() < 0.3:
            # ノイズ追加
            x = x + torch.randn_like(x) * 0.01
            edge_attr = edge_attr + torch.randn_like(edge_attr) * 0.01
        
        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, global_attr=global_attr)

def parse_msp_file(msp_file_path):
    """MSPファイルを解析し、ID->マススペクトルのマッピングを返す（改良版）"""
    msp_data = {}
    current_id = None
    current_peaks = []
    
    with open(msp_file_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            
            # IDを検出
            if line.startswith("ID:"):
                current_id = line.split(":")[1].strip()
                current_id = int(current_id)
            
            # ピーク数を検出（これはピークデータの直前にある）
            elif line.startswith("Num peaks:"):
                current_peaks = []
            
            # 空行は化合物の区切り
            elif line == "" and current_id is not None and current_peaks:
                # マススペクトルをベクトルに変換
                ms_vector = np.zeros(MAX_MZ)
                for mz, intensity in current_peaks:
                    if 0 <= mz < MAX_MZ:
                        ms_vector[mz] = intensity
                
                # 強度を正規化
                if np.sum(ms_vector) > 0:
                    ms_vector = ms_vector / np.max(ms_vector) * 100
                    
                    # スペクトルをスムージング
                    smoothed_vector = np.zeros_like(ms_vector)
                    for i in range(len(ms_vector)):
                        start = max(0, i-1)
                        end = min(len(ms_vector), i+2)
                        smoothed_vector[i] = np.mean(ms_vector[start:end])
                    
                    # 小さなピークをフィルタリング (ノイズ除去)
                    threshold = np.percentile(smoothed_vector[smoothed_vector > 0], 10)
                    smoothed_vector[smoothed_vector < threshold] = 0
                    
                    # 重要なm/z値のピークを強調
                    for mz in IMPORTANT_MZ:
                        if mz < len(smoothed_vector) and smoothed_vector[mz] > 0:
                            smoothed_vector[mz] *= 1.5
                    
                    msp_data[current_id] = smoothed_vector
                else:
                    msp_data[current_id] = ms_vector
                
                current_id = None
                current_peaks = []
            
            # ピークデータを処理
            elif current_id is not None and " " in line and not any(line.startswith(prefix) for prefix in ["Name:", "Formula:", "MW:", "ExactMass:", "CASNO:", "Comment:"]):
                try:
                    parts = line.split()
                    if len(parts) == 2:
                        mz = int(parts[0])
                        intensity = float(parts[1])
                        current_peaks.append((mz, intensity))
                except ValueError:
                    pass  # 数値に変換できない行はスキップ
    
    return msp_data

class AttentionBlock(nn.Module):
    """拡張グローバルアテンションブロック"""
    def __init__(self, in_dim, hidden_dim, heads=4):
        super(AttentionBlock, self).__init__()
        self.heads = heads
        self.head_dim = hidden_dim // heads
        
        # マルチヘッドアテンション
        self.query = nn.Linear(in_dim, hidden_dim)
        self.key = nn.Linear(in_dim, hidden_dim)
        self.value = nn.Linear(in_dim, hidden_dim)
        
        self.attn_combine = nn.Linear(hidden_dim, hidden_dim)
        
        # ゲート付きグローバルアテンション
        self.gate_nn = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.message_nn = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, in_dim)
        )
        
        # 出力投影
        self.out_proj = nn.Linear(hidden_dim + in_dim, in_dim)
        self.layer_norm = nn.LayerNorm(in_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x, batch):
        batch_size = torch.max(batch).item() + 1
        device = x.device
        
        # マルチヘッドアテンション計算
        q = self.query(x).view(-1, self.heads, self.head_dim)
        k = self.key(x).view(-1, self.heads, self.head_dim)
        v = self.value(x).view(-1, self.heads, self.head_dim)
        
        # バッチごとにアテンション計算
        attn_outputs = []
        
        for b in range(batch_size):
            mask = (batch == b)
            if not mask.any():
                continue
                
            q_b = q[mask]
            k_b = k[mask]
            v_b = v[mask]
            
            # スケーリングドットプロダクトアテンション
            attention = torch.bmm(q_b, k_b.transpose(1, 2)) / (self.head_dim ** 0.5)
            attention = F.softmax(attention, dim=2)
            
            # アテンション適用
            context = torch.bmm(attention, v_b)
            context = context.reshape(context.size(0), -1)
            context = self.attn_combine(context)
            
            attn_outputs.append(context)
        
        # アテンション出力の結合
        if attn_outputs:
            attn_out = torch.cat(attn_outputs, dim=0)
        else:
            attn_out = torch.zeros(x.size(0), self.heads * self.head_dim, device=device)
        
        # GlobalAttentionの計算
        global_attn = GlobalAttention(self.gate_nn, self.message_nn)(x, batch)
        
        # 結合と残差接続
        combined = torch.cat([attn_out, global_attn], dim=1)
        out = self.out_proj(combined)
        out = self.layer_norm(out + global_attn)  # 残差接続
        out = self.dropout(out)
        
        return out

class ResidualBlock(nn.Module):
    """残差ブロック（バッチ正規化付き）"""
    def __init__(self, in_channels, out_channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Linear(in_channels, out_channels)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.conv2 = nn.Linear(out_channels, out_channels)
        self.bn2 = nn.BatchNorm1d(out_channels)
        
        # 入力と出力のチャネル数が異なる場合の調整用レイヤー
        self.shortcut = nn.Sequential()
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Linear(in_channels, out_channels),
                nn.BatchNorm1d(out_channels)
            )
            
        # Squeeze-and-Excitation
        self.se = SqueezeExcitation(out_channels)
        
        # ドロップアウト
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        residual = self.shortcut(x)
        
        out = F.leaky_relu(self.bn1(self.conv1(x)))
        out = self.dropout(out)
        out = self.bn2(self.conv2(out))
        
        # SE適用
        out = self.se(out)
        
        out += residual  # 残差接続
        out = F.leaky_relu(out)
        
        return out

class SqueezeExcitation(nn.Module):
    """Squeeze-and-Excitation ブロック"""
    def __init__(self, channel, reduction=16):
        super(SqueezeExcitation, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, max(channel // reduction, 8), bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(max(channel // reduction, 8), channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c = x.size()
        y = self.avg_pool(x.unsqueeze(-1)).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.squeeze(-1)

class ConvBlock1D(nn.Module):
    """1D畳み込みブロック（スペクトル処理用）"""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super(ConvBlock1D, self).__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)
        self.bn = nn.BatchNorm1d(out_channels)
        self.act = nn.LeakyReLU()
        
    def forward(self, x):
        # 入力形状: [batch_size, channels, feature_dim]
        x = self.conv(x)
        x = self.bn(x)
        x = self.act(x)
        return x

class HybridGNNModel(nn.Module):
    """ハイブリッドGNN + CNN + Transformerモデル（コサイン類似度0.9目標）"""
    def __init__(self, node_features, edge_features, hidden_channels, out_channels, num_fragments=NUM_FRAGS):
        super(HybridGNNModel, self).__init__()
        
        # エッジの実際の次元を計算（edge_featuresはエッジ特徴量の基本次元）
        edge_dim = edge_features
        print(f"エッジ特徴量の次元: {edge_dim}")
        
        # グローバル特徴量の次元を設定 - グラフ単位の特徴量次元は16
        self.global_features_dim = 16  # 1分子あたりの特徴量次元
        print(f"グローバル特徴量の次元: {self.global_features_dim}")
        
        # グラフアテンション層（マルチヘッド）
        self.conv1 = GATv2Conv(node_features, hidden_channels, edge_dim=edge_dim, heads=4)
        self.conv2 = GATv2Conv(hidden_channels*4, hidden_channels, edge_dim=edge_dim, heads=4)
        self.conv3 = GATv2Conv(hidden_channels*4, hidden_channels, edge_dim=edge_dim, heads=4)
        self.conv4 = GATv2Conv(hidden_channels*4, hidden_channels, edge_dim=edge_dim, heads=4)
        self.conv5 = GATv2Conv(hidden_channels*4, hidden_channels, edge_dim=edge_dim, heads=2)
        
        # スキップ接続
        self.skip_connection1 = nn.Linear(hidden_channels*4, hidden_channels*4)
        self.skip_connection2 = nn.Linear(hidden_channels*4, hidden_channels*4)
        
        # グローバルアテンション
        self.global_attention = AttentionBlock(hidden_channels*2, hidden_channels, heads=4)
        
        # グローバル特徴量処理
        self.global_proj = nn.Sequential(
            nn.Linear(self.global_features_dim, hidden_channels*2),
            nn.LeakyReLU(),
            nn.LayerNorm(hidden_channels*2)
        )
        
        # スペクトル予測のための全結合層
        self.fc_layers = nn.ModuleList([
            ResidualBlock(hidden_channels*2 + hidden_channels*2, hidden_channels*4),
            ResidualBlock(hidden_channels*4, hidden_channels*4),
            ResidualBlock(hidden_channels*4, hidden_channels*4),
            ResidualBlock(hidden_channels*4, hidden_channels*2)
        ])
        
        # マルチタスク学習: フラグメントパターン予測
        self.fragment_pred = nn.Sequential(
            nn.Linear(hidden_channels*2, hidden_channels),
            nn.LeakyReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_channels, num_fragments),
        )
        
        # 最終出力層（スペクトル予測）
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_channels*2, hidden_channels),
            nn.LeakyReLU(),
            nn.Linear(hidden_channels, MAX_MZ)
        )
        
        # 活性化関数とドロップアウト
        self.act = nn.LeakyReLU()
        self.dropout = nn.Dropout(0.3)
        
        # バッチ正規化
        self.bn1 = nn.BatchNorm1d(hidden_channels*4)
        self.bn2 = nn.BatchNorm1d(hidden_channels*4)
        self.bn3 = nn.BatchNorm1d(hidden_channels*4)
        self.bn4 = nn.BatchNorm1d(hidden_channels*2)
        
        # レイヤー正規化
        self.ln1 = nn.LayerNorm(hidden_channels*4)
        
        # 重み初期化
        self._init_weights()
    
    def _init_weights(self):
        """重みの初期化（収束を高速化）"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, data):
        # デバッグ情報
        print(f"入力データの形状:")
        print(f"ノード特徴量 x: {data.x.shape}")
        if hasattr(data, 'edge_attr') and data.edge_attr is not None:
            print(f"エッジ特徴量 edge_attr: {data.edge_attr.shape}")
        print(f"エッジインデックス edge_index: {data.edge_index.shape}")
        print(f"バッチ: {data.batch.shape}")
        print(f"グローバル属性: {data.global_attr.shape}")
        
        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
        global_attr = data.global_attr
        
        # グローバル属性のリシェイプとバッチサイズ調整 - バッチサイズに対応
        if len(global_attr.shape) == 1:  # [batch_size*features] -> [batch_size, features]
            num_graphs = batch[-1].item() + 1  # バッチ内のグラフ数
            features_per_graph = global_attr.shape[0] // num_graphs
            
            # グローバル属性をバッチサイズに合わせてリシェイプ
            if features_per_graph == self.global_features_dim:
                # 正常なケース: 各グラフが16次元の特徴を持つ
                global_attr = global_attr.view(num_graphs, self.global_features_dim)
            else:
                # 異常なケース: 特徴量の次元が期待と異なる
                # 特徴量をゼロパディングして対応
                print(f"Warning: グローバル特徴量の次元が予期せぬ値です: {features_per_graph}")
                global_attr_padded = torch.zeros(num_graphs, self.global_features_dim, device=global_attr.device)
                # 利用可能な特徴量をコピー
                for i in range(num_graphs):
                    start_idx = i * features_per_graph
                    end_idx = min(start_idx + features_per_graph, global_attr.shape[0])
                    copy_size = min(features_per_graph, self.global_features_dim)
                    global_attr_padded[i, :copy_size] = global_attr[start_idx:start_idx+copy_size]
                global_attr = global_attr_padded
        
        # グラフコンボリューション層
        x1 = self.act(self.conv1(x, edge_index, edge_attr))
        x1 = self.dropout(x1)
        
        x2 = self.act(self.conv2(x1, edge_index, edge_attr))
        x2 = self.dropout(x2)
        
        # スキップ接続1
        x1_transformed = self.skip_connection1(x1)
        x2 = x2 + x1_transformed
        x2 = self.ln1(x2)
        
        x3 = self.act(self.conv3(x2, edge_index, edge_attr))
        x3 = self.dropout(x3)
        
        x4 = self.act(self.conv4(x3, edge_index, edge_attr))
        x4 = self.dropout(x4)
        
        # スキップ接続2
        x2_transformed = self.skip_connection2(x2)
        x4 = x4 + x2_transformed
        
        x5 = self.act(self.conv5(x4, edge_index, edge_attr))
        
        # グローバルプーリングを使用してノード特徴量をグラフ単位に集約
        x_graph = global_mean_pool(x5, batch)
        
        # グローバル特徴量の処理
        global_features = self.global_proj(global_attr)
        
        # 特徴量の結合
        x_combined = torch.cat([x_graph, global_features], dim=1)
        
        # 残差ブロックを通した特徴抽出
        for i, fc_layer in enumerate(self.fc_layers):
            x_combined = fc_layer(x_combined)
            if i < len(self.fc_layers) - 1:
                x_combined = self.dropout(x_combined)
        
        # マルチタスク学習: フラグメントパターン予測
        fragment_pred = self.fragment_pred(x_combined)
        
        # スペクトル予測
        output = self.output_layer(x_combined)
        
        # 出力次元が合わない場合は調整する
        if output.shape[1] != MAX_MZ:
            print(f"警告: 出力サイズを {output.shape[1]} から {MAX_MZ} に調整します")
            new_output = torch.zeros(output.shape[0], MAX_MZ, device=output.device)
            min_size = min(output.shape[1], MAX_MZ)
            new_output[:, :min_size] = output[:, :min_size]
            output = new_output
        
        # 出力の正規化（非負制約）
        output = F.relu(output)
        
        return output, fragment_pred

def fragment_pattern_loss(y_pred, y_true, top_k=30):
    """フラグメントパターンを重視した損失関数（修正版）"""
    batch_size = y_pred.shape[0]
    device = y_pred.device
    
    # 各スペクトルの上位k個のピークを抽出
    k_pred = min(top_k, y_pred.size(1))
    k_true = min(top_k, y_true.size(1))
    _, pred_top_indices = torch.topk(y_pred, k=k_pred, dim=1)
    _, true_top_indices = torch.topk(y_true, k=k_true, dim=1)
    
    loss = 0.0
    valid_samples = 0
    
    for i in range(batch_size):
        # 予測と実測の上位ピーク位置
        pred_peaks = pred_top_indices[i]
        true_peaks = true_top_indices[i]
        
        # 予測と実測のピーク位置の一致度を評価（集合演算）
        pred_set = set(pred_peaks.cpu().numpy())
        true_set = set(true_peaks.cpu().numpy())
        
        common_peaks = len(pred_set.intersection(true_set))
        union_peaks = len(pred_set.union(true_set))
        
        jaccard = common_peaks / max(1, union_peaks)
        
        # ピーク間の距離による評価
        total_dist = 0.0
        count = 0
        
        for p in pred_peaks:
            p_val = p.item()
            min_dist = float('inf')
            for t in true_peaks:
                t_val = t.item()
                dist = abs(p_val - t_val)
                min_dist = min(min_dist, dist)
            
            if min_dist != float('inf'):
                total_dist += min_dist
                count += 1
        
        # 平均距離を計算
        if count > 0:
            mean_distance = total_dist / count
            dist_factor = math.exp(-mean_distance / 10.0)  # Python の math.exp を使用
        else:
            dist_factor = 0.0
            
        # パターン類似性損失
        pattern_loss = 1.0 - (jaccard * dist_factor)
        loss += pattern_loss
        valid_samples += 1
    
    if valid_samples > 0:
        return loss / valid_samples
    else:
        return torch.tensor(1.0, device=device)  # 有効なサンプルがない場合

def relative_intensity_loss(y_pred, y_true, top_k=30):
    """フラグメントイオン間の相対強度比を保存するための損失関数（修正版）"""
    batch_size = y_pred.shape[0]
    device = y_pred.device
    loss = 0.0
    valid_samples = 0
    
    for i in range(batch_size):
        pred = y_pred[i]
        true = y_true[i]
        
        # 上位kピークを抽出（非ゼロ値の数とtop_kの小さい方）
        nonzero_count = torch.count_nonzero(true)
        if nonzero_count < 2:
            continue
            
        k = min(top_k, nonzero_count.item())
        true_values, true_indices = torch.topk(true, k=k)
        
        # 実測データに重要なピークがあるか確認
        if torch.sum(true_values > 0) < 2:
            continue
            
        # ピーク間の強度比の計算
        ratios_true = []
        ratios_pred = []
        
        # 各ピアのピーク強度比を計算
        for idx1 in range(len(true_indices)):
            for idx2 in range(idx1+1, len(true_indices)):
                m1, m2 = true_indices[idx1].item(), true_indices[idx2].item()
                
                # 実測値の強度
                i1_true = true[m1].item()
                i2_true = true[m2].item()
                
                # どちらかが0なら無視
                if i1_true <= 0 or i2_true <= 0:
                    continue
                    
                # 予測値の強度
                i1_pred = pred[m1].item()
                i2_pred = pred[m2].item()
                
                # 負の値や極小値のチェック
                if i1_pred <= 1e-6 or i2_pred <= 1e-6:
                    continue
                
                # 強度比（大きい方を分子に）
                if i1_true >= i2_true:
                    ratio_true = i1_true / (i2_true + 1e-6)
                    ratio_pred = i1_pred / (i2_pred + 1e-6)
                else:
                    ratio_true = i2_true / (i1_true + 1e-6)
                    ratio_pred = i2_pred / (i1_pred + 1e-6)
                
                ratios_true.append(ratio_true)
                ratios_pred.append(ratio_pred)
        
        # 強度比の差を損失として計算
        if ratios_true:
            # 警告を解消するために改善された方法でテンソルを作成
            ratios_true_tensor = torch.tensor(ratios_true, device=device)
            ratios_pred_tensor = torch.tensor(ratios_pred, device=device)
            
            # 比率の対数差（比率の違いを相対的に評価）
            log_true = torch.log(ratios_true_tensor + 1e-6)
            log_pred = torch.log(ratios_pred_tensor + 1e-6)
            log_diff = torch.abs(log_true - log_pred)
            
            ratio_loss = torch.mean(log_diff)
            loss += ratio_loss
            valid_samples += 1
    
    # バッチ内の有効なサンプル数で割って平均損失を計算
    if valid_samples > 0:
        return loss / valid_samples
    else:
        return torch.tensor(0.0, device=device)  # 有効なサンプルがない場合

def peak_weighted_cosine_loss(y_pred, y_true, important_mz=None, important_weight=3.0):
    """ピークと重要なm/z値を重視したコサイン類似度損失関数"""
    # 正規化
    y_pred_norm = F.normalize(y_pred, p=2, dim=1)
    y_true_norm = F.normalize(y_true, p=2, dim=1)
    
    # 特徴的なm/zの重み付け
    if important_mz is not None:
        batch_size = y_pred.size(0)
        weights = torch.ones_like(y_pred)
        for mz in important_mz:
            if mz < y_pred.size(1):
                weights[:, mz] = important_weight
        
        # 重み付きベクトルで正規化
        y_pred_weighted = y_pred * weights
        y_true_weighted = y_true * weights
        
        y_pred_norm = F.normalize(y_pred_weighted, p=2, dim=1)
        y_true_norm = F.normalize(y_true_weighted, p=2, dim=1)
    
    # コサイン類似度（-1〜1の範囲）
    cosine = torch.sum(y_pred_norm * y_true_norm, dim=1)
    
    # 損失を1 - cosineにして、0〜2の範囲に
    loss = 1.0 - cosine
    
    return loss.mean()

def focal_cosine_similarity_loss(y_pred, y_true, alpha=0.25, gamma=2.0, eps=1e-8):
    """フォーカルコサイン類似度損失
    
    難しいサンプルに集中する損失関数。コサイン類似度が低いサンプルに
    より高い重みを与え、学習を促進します。
    """
    # 正規化
    y_pred_norm = F.normalize(y_pred, p=2, dim=1)
    y_true_norm = F.normalize(y_true, p=2, dim=1)
    
    # コサイン類似度（-1〜1の範囲）
    cosine = torch.sum(y_pred_norm * y_true_norm, dim=1)
    
    # フォーカル重み（類似度が低いサンプルを強調）
    focal_weight = alpha * torch.pow(1.0 - cosine, gamma)
    
    # 損失を1 - cosineにして、0〜2の範囲に
    loss = 1.0 - cosine
    
    # 重み付き損失
    weighted_loss = focal_weight * loss
    
    return weighted_loss.mean()

def combined_loss(y_pred, y_true, fragment_pred=None, fragment_true=None, 
                 alpha=0.1, beta=0.4, gamma=0.2, delta=0.2, epsilon=0.1):
    """最適化された複合損失関数"""
    # バッチサイズのチェックと調整
    if y_pred.shape[0] != y_true.shape[0]:
        min_batch_size = min(y_pred.shape[0], y_true.shape[0])
        y_pred = y_pred[:min_batch_size]
        y_true = y_true[:min_batch_size]
    
    # 特徴数のチェックと調整
    if y_pred.shape[1] != y_true.shape[1]:
        min_size = min(y_pred.shape[1], y_true.shape[1])
        y_pred = y_pred[:, :min_size]
        y_true = y_true[:, :min_size]
    
    # 1. ピーク重視MSE損失
    peak_mask = (y_true > 0).float()
    mse_weights = peak_mask * 10.0 + 1.0
    
    # 重要なm/z値にさらに重みを付ける
    for mz in IMPORTANT_MZ:
        if mz < y_true.size(1):
            mse_weights[:, mz] *= 3.0
    
    mse_loss = torch.mean(mse_weights * (y_pred - y_true) ** 2)
    
    # 2. コサイン類似度損失
    cosine_loss = peak_weighted_cosine_loss(y_pred, y_true, important_mz=IMPORTANT_MZ)
    
    # 3. フラグメントパターン損失 - 計算コストが高いので重みを下げる
    pattern_loss = fragment_pattern_loss(y_pred, y_true, top_k=30)  # top_kを小さくして高速化
    
    # 4. 相対強度比保存損失 - 計算コストが高いので重みを下げる
    rel_intensity_loss_val = relative_intensity_loss(y_pred, y_true, top_k=20)  # top_kを小さくして高速化
    
    # 主要な損失関数の組み合わせ
    main_loss = alpha * mse_loss + beta * cosine_loss + gamma * pattern_loss + delta * rel_intensity_loss_val
    
    # フラグメントパターン予測がある場合
    if fragment_pred is not None and fragment_true is not None:
        if fragment_pred.shape[0] != fragment_true.shape[0]:
            min_batch_size = min(fragment_pred.shape[0], fragment_true.shape[0])
            fragment_pred = fragment_pred[:min_batch_size]
            fragment_true = fragment_true[:min_batch_size]
        
        fragment_loss = F.binary_cross_entropy_with_logits(fragment_pred, fragment_true)
        return main_loss + epsilon * fragment_loss
    
    return main_loss

def cosine_similarity_score(y_true, y_pred, important_mz=IMPORTANT_MZ):
    """改良されたコサイン類似度スコア計算（フラグメントパターンと相対強度比評価を含む）"""
    # NumPy配列に変換
    y_true_np = y_true.cpu().numpy() if isinstance(y_true, torch.Tensor) else y_true
    y_pred_np = y_pred.cpu().numpy() if isinstance(y_pred, torch.Tensor) else y_pred
    
    y_true_flat = y_true_np.reshape(y_true_np.shape[0], -1)
    y_pred_flat = y_pred_np.reshape(y_pred_np.shape[0], -1)
    
    # 上位K個のピークのみを考慮
    K = 200
    
    scores = []
    pattern_scores = []  # フラグメントパターンスコア
    rel_intensity_scores = []  # 相対強度比スコア
    
    for i in range(y_true_flat.shape[0]):
        true_vec = y_true_flat[i]
        pred_vec = y_pred_flat[i]
        
        # ゼロベクターチェック
        if np.sum(true_vec) == 0 or np.sum(pred_vec) == 0:
            scores.append(0)
            pattern_scores.append(0)
            rel_intensity_scores.append(0)
            continue
        
        # 1. 基本コサイン類似度
        cosine_sim = cosine_similarity(
            true_vec.reshape(1, -1), 
            pred_vec.reshape(1, -1)
        )[0][0]
        
        # 2. フラグメントパターン類似度
        # 上位Kピークを抽出
        true_top_indices = np.argsort(true_vec)[-K:]
        pred_top_indices = np.argsort(pred_vec)[-K:]
        
        # Jaccard類似度: 共通ピーク数 / 全ピーク数
        common_peaks = len(set(true_top_indices) & set(pred_top_indices))
        total_peaks = len(set(true_top_indices) | set(pred_top_indices))
        pattern_sim = common_peaks / total_peaks if total_peaks > 0 else 0
        
        # 3. 相対強度比の評価
        if len(true_top_indices) >= 5:
            # 上位5ピークを使用
            top5_true = np.argsort(true_vec)[-5:]
            ratios_true = []
            ratios_pred = []
            
            for idx1 in range(len(top5_true)):
                for idx2 in range(idx1+1, len(top5_true)):
                    m1, m2 = top5_true[idx1], top5_true[idx2]
                    
                    # 実測値の強度
                    i1_true, i2_true = true_vec[m1], true_vec[m2]
                    
                    # 予測値の強度
                    i1_pred, i2_pred = pred_vec[m1], pred_vec[m2]
                    
                    if i1_true > 0 and i2_true > 0 and i1_pred > 0 and i2_pred > 0:
                        # 強度比（大きい方を分子に）
                        if i1_true >= i2_true:
                            ratio_true = i1_true / i2_true
                            ratio_pred = i1_pred / i2_pred
                        else:
                            ratio_true = i2_true / i1_true
                            ratio_pred = i2_pred / i1_pred
                        
                        # 比率の一致度（1.0が完全一致）
                        ratio_match = min(ratio_true, ratio_pred) / max(ratio_true, ratio_pred)
                        ratios_true.append(ratio_true)
                        ratios_pred.append(ratio_pred)
            
            # 相対強度比のスコア計算
            if ratios_true:
                rel_intensity_score = np.mean([min(r1, r2) / max(r1, r2) for r1, r2 in zip(ratios_true, ratios_pred)])
                rel_intensity_scores.append(rel_intensity_score)
            else:
                rel_intensity_scores.append(0)
        else:
            rel_intensity_scores.append(0)
        
        # 4. 複合スコアの計算
        # コサイン類似度(60%)、パターン類似度(20%)、相対強度比(20%)の加重平均
        pattern_sim_value = pattern_sim if not np.isnan(pattern_sim) else 0
        rel_intensity_value = rel_intensity_scores[-1] if rel_intensity_scores else 0
        
        composite_score = 0.6 * cosine_sim + 0.2 * pattern_sim_value + 0.2 * rel_intensity_value
        scores.append(composite_score)
        pattern_scores.append(pattern_sim)
    
    # 各スコアの平均を表示
    avg_cosine = np.mean(scores)
    avg_pattern = np.mean(pattern_scores) if pattern_scores else 0
    avg_rel_intensity = np.mean(rel_intensity_scores) if rel_intensity_scores else 0
    
    print(f"評価指標: コサイン類似度={avg_cosine:.4f}, パターン類似度={avg_pattern:.4f}, 相対強度比={avg_rel_intensity:.4f}")
    
    return avg_cosine  # 互換性のため、コサイン類似度を返す

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):
    """モデルの学習を行う（改良版）"""
    train_losses = []
    val_losses = []
    val_cosine_similarities = []
    best_cosine = 0.0
    early_stopping_counter = 0
    early_stopping_patience = 15
    
    for epoch in range(num_epochs):
        # 訓練モード
        model.train()
        epoch_loss = 0
        batch_count = 0
        
        for graph_data, mass_spectrum, fragment_pattern in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} (Training)"):
            try:
                # データをGPUに転送
                graph_data = graph_data.to(device)
                mass_spectrum = mass_spectrum.to(device)
                fragment_pattern = fragment_pattern.to(device)
                
                # バッチサイズの確認
                batch_size = graph_data.batch[-1].item() + 1 if hasattr(graph_data, 'batch') and graph_data.batch.numel() > 0 else 1
                print(f"Current batch size: {batch_size}")
                
                # 勾配をゼロに初期化
                optimizer.zero_grad()
                
                # 順伝播
                output, fragment_pred = model(graph_data)
                
                # テンソルのサイズを表示（デバッグ用）
                print(f"Output shape: {output.shape}, Mass spectrum shape: {mass_spectrum.shape}")
                print(f"Fragment pred shape: {fragment_pred.shape}, Fragment pattern shape: {fragment_pattern.shape}")
                
                # バッチサイズの不一致を処理
                if output.size(0) != mass_spectrum.size(0):
                    print(f"バッチサイズの不一致を検出: output={output.size(0)}, mass_spectrum={mass_spectrum.size(0)}")
                    min_batch_size = min(output.size(0), mass_spectrum.size(0))
                    output = output[:min_batch_size]
                    mass_spectrum = mass_spectrum[:min_batch_size]
                    fragment_pred = fragment_pred[:min_batch_size]
                    fragment_pattern = fragment_pattern[:min_batch_size]
                    print(f"バッチサイズ調整後: output={output.shape}, mass_spectrum={mass_spectrum.shape}")
                
                # 損失計算
                loss = criterion(output, mass_spectrum, fragment_pred, fragment_pattern)
                
                # 逆伝播
                loss.backward()
                
                # 勾配クリッピング（勾配爆発を防止）
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
                
            except RuntimeError as e:
                print(f"エラーが発生しました: {str(e)}")
                print(f"バッチサイズ: {batch_size}")
                print(f"グラフデータ: {graph_data}")
                continue
        
        # バッチ処理が成功したか確認
        if batch_count > 0:
            train_losses.append(epoch_loss / batch_count)
        else:
            print("警告：このエポックで成功したバッチ処理がありません。")
            train_losses.append(float('inf'))
        
        # 評価モード
        model.eval()
        val_loss = 0
        val_batch_count = 0
        y_true = []
        y_pred = []
        
        with torch.no_grad():
            for graph_data, mass_spectrum, fragment_pattern in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} (Validation)"):
                try:
                    graph_data = graph_data.to(device)
                    mass_spectrum = mass_spectrum.to(device)
                    fragment_pattern = fragment_pattern.to(device)
                    
                    output, fragment_pred = model(graph_data)
                    
                    # バッチサイズの不一致を処理
                    if output.size(0) != mass_spectrum.size(0):
                        min_batch_size = min(output.size(0), mass_spectrum.size(0))
                        output = output[:min_batch_size]
                        mass_spectrum = mass_spectrum[:min_batch_size]
                        fragment_pred = fragment_pred[:min_batch_size]
                        fragment_pattern = fragment_pattern[:min_batch_size]
                    
                    loss = criterion(output, mass_spectrum, fragment_pred, fragment_pattern)
                    val_loss += loss.item()
                    val_batch_count += 1
                    
                    y_true.append(mass_spectrum.cpu().numpy())
                    y_pred.append(output.cpu().numpy())
                    
                except RuntimeError as e:
                    print(f"評価中にエラーが発生しました: {str(e)}")
                    continue
        
        # バッチ処理が成功したか確認
        if val_batch_count > 0:
            val_losses.append(val_loss / val_batch_count)
        else:
            print("警告：評価中に成功したバッチ処理がありません。")
            val_losses.append(float('inf'))
        
        # コサイン類似度を計算
        if y_true and y_pred:  # リストが空でないことを確認
            try:
                y_true_concat = np.concatenate(y_true)
                y_pred_concat = np.concatenate(y_pred)
                cosine_sim = cosine_similarity_score(y_true_concat, y_pred_concat)
                val_cosine_similarities.append(cosine_sim)
            except Exception as e:
                print(f"コサイン類似度計算中にエラーが発生しました: {str(e)}")
                val_cosine_similarities.append(0.0)
                cosine_sim = 0.0
        else:
            val_cosine_similarities.append(0.0)
            cosine_sim = 0.0
        
        # 学習率スケジューラーの更新
        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(val_losses[-1])
        else:
            scheduler.step()
        
        print(f"Epoch {epoch+1}/{num_epochs}, "
              f"Train Loss: {train_losses[-1]:.4f}, "
              f"Val Loss: {val_losses[-1]:.4f}, "
              f"Val Cosine Similarity: {cosine_sim:.4f}, "
              f"LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # 最良モデルの保存
        if cosine_sim > best_cosine:
            best_cosine = cosine_sim
            early_stopping_counter = 0
            torch.save(model.state_dict(), 'best_gcn_ms_model.pth')
            print(f"新しい最良モデル保存: {cosine_sim:.4f}")
        else:
            early_stopping_counter += 1
            
        # 早期停止
        if early_stopping_counter >= early_stopping_patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break
    
    return train_losses, val_losses, val_cosine_similarities

def collate_fn(batch):
    """バッチ内のグラフデータとマススペクトルを結合する"""
    graphs, spectra, fragments = zip(*batch)
    batched_graphs = Batch.from_data_list(list(graphs))
    batched_spectra = torch.stack(spectra)
    batched_fragments = torch.stack(fragments)
    return batched_graphs, batched_spectra, batched_fragments

class ModelEnsemble:
    """複数モデルのアンサンブル"""
    def __init__(self, models, device):
        self.models = models
        self.device = device
        
    def __call__(self, data):
        """複数モデルの予測平均を返す"""
        outputs = []
        fragment_outputs = []
        
        data = data.to(self.device)
        with torch.no_grad():
            for model in self.models:
                model.eval()
                output, fragment_pred = model(data)
                outputs.append(output)
                fragment_outputs.append(fragment_pred)
        
        # 出力の平均
        mean_output = torch.mean(torch.stack(outputs), dim=0)
        mean_fragment = torch.mean(torch.stack(fragment_outputs), dim=0)
        
        return mean_output, mean_fragment

def create_ensemble_from_checkpoints(model_class, checkpoint_paths, node_features, edge_features, hidden_channels, out_channels, device):
    """チェックポイントからアンサンブルモデルを作成"""
    models = []
    
    for path in checkpoint_paths:
        try:
            model = model_class(node_features, edge_features, hidden_channels, out_channels)
            model.load_state_dict(torch.load(path))
            model.to(device)
            model.eval()
            models.append(model)
            print(f"モデルを読み込みました: {path}")
        except Exception as e:
            print(f"モデルの読み込みに失敗しました {path}: {e}")
    
    return ModelEnsemble(models, device)

def debug_mol_to_graph(mol_file):
    """分子をグラフに変換し、デバッグ情報を表示する"""
    # RDKitでMOLファイルを読み込む (サニタイズを無効にして読み込む)
    mol = Chem.MolFromMolFile(mol_file, sanitize=False)
    if mol is None:
        raise ValueError(f"Could not read molecule from {mol_file}")
    
    # 拡張機能: 特徴量の追加
    try:
        # プロパティキャッシュを更新して暗黙的な原子価を計算
        for atom in mol.GetAtoms():
            atom.UpdatePropertyCache(strict=False)
        
        # 部分的なサニタイズ
        Chem.SanitizeMol(mol, 
                       sanitizeOps=Chem.SanitizeFlags.SANITIZE_FINDRADICALS|
                                  Chem.SanitizeFlags.SANITIZE_KEKULIZE|
                                  Chem.SanitizeFlags.SANITIZE_SETAROMATICITY|
                                  Chem.SanitizeFlags.SANITIZE_SETCONJUGATION|
                                  Chem.SanitizeFlags.SANITIZE_SETHYBRIDIZATION|
                                  Chem.SanitizeFlags.SANITIZE_SYMMRINGS,
                       catchErrors=True)
    except Exception as e:
        print(f"分子のサニタイズエラー: {str(e)}")
    
    # 原子情報を取得
    num_atoms = mol.GetNumAtoms()
    x = []
    
    # 原子特徴量を抽出
    for atom in mol.GetAtoms():
        # 原子特徴量を計算（簡略化して表示）
        atom_symbol = atom.GetSymbol()
        atom_feature_idx = ATOM_FEATURES.get(atom_symbol, ATOM_FEATURES['OTHER'])
        atom_feature = [0] * len(ATOM_FEATURES)
        atom_feature[atom_feature_idx] = 1
        
        # 追加特徴量の計算
        additional_features = [0.0] * 15  # 追加特徴量の数
        
        # すべての特徴を結合
        atom_feature.extend(additional_features)
        x.append(atom_feature)
    
    # 結合情報を取得
    edge_indices = []
    edge_attrs = []
    
    for bond in mol.GetBonds():
        try:
            i = bond.GetBeginAtomIdx()
            j = bond.GetEndAtomIdx()
            
            # 結合タイプ
            bond_type = BOND_FEATURES.get(bond.GetBondType(), BOND_FEATURES[Chem.rdchem.BondType.SINGLE])
            
            # 双方向のエッジを追加
            edge_indices.append([i, j])
            edge_indices.append([j, i])
            
            # ボンド特徴量
            bond_feature = [0] * len(BOND_FEATURES)
            bond_feature[bond_type] = 1
            
            # 安全な追加ボンド特徴量の取得
            additional_bond_features = [0.0] * 6  # 追加ボンド特徴量の数
            
            bond_feature.extend(additional_bond_features)
            edge_attrs.append(bond_feature)
            edge_attrs.append(bond_feature)  # 双方向なので同じ属性
        except Exception as e:
            print(f"結合処理エラー: {str(e)}")
            continue
    
    # 分子全体の特徴量
    mol_features = [0.0] * 16  # デフォルト値で初期化
    
    # デバッグ情報の表示
    print(f"分子: {mol_file}")
    print(f"原子数: {num_atoms}")
    print(f"ノード特徴量次元: {len(x[0]) if x else 0}")
    print(f"エッジ数: {len(edge_indices) // 2}")  # 双方向なので2で割る
    print(f"エッジ特徴量次元: {len(edge_attrs[0]) if edge_attrs else 0}")
    
    # エッジ特徴量の詳細を表示
    if edge_attrs:
        print(f"エッジ特徴量の最初の例: {edge_attrs[0]}")
        # 結合タイプの部分（最初の4つ）
        print(f"  - 結合タイプ: {edge_attrs[0][:len(BOND_FEATURES)]}")
        # 追加特徴量の部分
        print(f"  - 追加特徴量: {edge_attrs[0][len(BOND_FEATURES):]}")
    
    # グラフデータを作成
    x = torch.FloatTensor(x)
    edge_index = torch.LongTensor(edge_indices).t().contiguous()
    edge_attr = torch.FloatTensor(edge_attrs)
    global_attr = torch.FloatTensor(mol_features)
    
    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, global_attr=global_attr)

# データセットのサンプルをチェック
def check_dataset_samples(dataset, num_samples=3):
    """データセットのサンプルをチェックしてデバッグ情報を表示"""
    print("\nデータセットのサンプルチェック:")
    for i in range(min(num_samples, len(dataset))):
        graph_data, mass_spectrum, fragment_pattern = dataset[i]
        print(f"\nサンプル {i+1}/{num_samples}:")
        print(f"グラフデータ:")
        print(f"  - ノード特徴量: {graph_data.x.shape}")
        print(f"  - エッジインデックス: {graph_data.edge_index.shape}")
        print(f"  - エッジ特徴量: {graph_data.edge_attr.shape}")
        print(f"  - グローバル特徴量: {graph_data.global_attr.shape}")
        print(f"質量スペクトル: {mass_spectrum.shape}")
        print(f"フラグメントパターン: {fragment_pattern.shape}")

def main():
    # MSPファイルを解析
    print("MSPファイルを解析中...")
    msp_data = parse_msp_file(MSP_FILE_PATH)
    print(f"MSPファイルから{len(msp_data)}個の化合物データを読み込みました")
    
    # 利用可能なMOLファイルを確認
    mol_ids = []
    for filename in os.listdir(MOL_FILES_PATH):
        if filename.startswith("ID") and filename.endswith(".MOL"):
            mol_id = int(filename[2:-4])  # "ID300001.MOL" → 300001
            if mol_id in msp_data:
                mol_ids.append(mol_id)
    
    print(f"MOLファイルとMSPデータが揃っている化合物: {len(mol_ids)}個")
    
    # データを分割（訓練:検証:テスト = 85:5:10）
    train_ids, test_ids = train_test_split(mol_ids, test_size=0.15, random_state=42)
    val_ids, test_ids = train_test_split(test_ids, test_size=0.67, random_state=42)
    
    print(f"訓練データ: {len(train_ids)}個")
    print(f"検証データ: {len(val_ids)}個")
    print(f"テストデータ: {len(test_ids)}個")
    
    # データセットとデータローダーを作成（訓練データにはデータ拡張を適用）
    print("データセットを準備中...")
    train_dataset = MoleculeGraphDataset(train_ids, MOL_FILES_PATH, msp_data, augment=True)
    val_dataset = MoleculeGraphDataset(val_ids, MOL_FILES_PATH, msp_data, augment=False)
    test_dataset = MoleculeGraphDataset(test_ids, MOL_FILES_PATH, msp_data, augment=False)
    
    print(f"有効な訓練データ: {len(train_dataset)}個")
    print(f"有効な検証データ: {len(val_dataset)}個")
    print(f"有効なテストデータ: {len(test_dataset)}個")
    
    # データセットが空でないか確認
    if len(train_dataset) == 0 or len(val_dataset) == 0 or len(test_dataset) == 0:
        print("エラー: 有効なデータが不足しています。処理を終了します。")
        return
    
    # バッチサイズを大きくして学習を効率化
    batch_size = 64
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    # サンプルの特徴量次元を確認
    sample_data = train_dataset[0][0]
    node_features = sample_data.x.size(1)
    # 重要: 実際のエッジ特徴量の次元を取得する
    edge_features = sample_data.edge_attr.size(1)
    hidden_channels = 256
    out_channels = MAX_MZ
    
    print(f"ノード特徴量次元: {node_features}")
    print(f"エッジ特徴量次元: {edge_features}")
    
    # デバイスの設定
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # GPU使用効率の向上
    if torch.cuda.is_available():
        # GPUメモリの使用効率を高める
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False  # 再現性よりも速度を優先
        
        # GPUの現在の状態を表示
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        
        # メモリ使用量を監視（オプション）
        torch.cuda.reset_peak_memory_stats()
    
    # モデルアンサンブル用のチェックポイントリスト
    model_checkpoints = []
    
    # 複数のモデルをトレーニング（アンサンブル用）
    num_models = 5  # アンサンブルするモデル数
    
    for model_idx in range(num_models):
        print(f"\nトレーニングモデル {model_idx+1}/{num_models}")
        
        # モデル、損失関数、オプティマイザーの初期化
        seed = 42 + model_idx
        torch.manual_seed(seed)
        random.seed(seed)
        np.random.seed(seed)
        
        try:
            # 修正: GATv2Convのedge_dimパラメータには実際のエッジ特徴量の次元を渡す
            model = HybridGNNModel(
                node_features, 
                edge_features,  # +6を追加しない
                hidden_channels, 
                MAX_MZ
            ).to(device)
            
            # 複合損失関数の設定（コサイン類似度重視）
            criterion = combined_loss
            
            # オプティマイザーの設定（AdamWを使用）
            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)
            
            # 学習率スケジューラー（OneCycleLRを使用）
            num_epochs = 100
            steps_per_epoch = len(train_loader)
            scheduler = torch.optim.lr_scheduler.OneCycleLR(
                optimizer,
                max_lr=0.003,
                steps_per_epoch=steps_per_epoch,
                epochs=num_epochs,
                pct_start=0.1,  # 10%の期間でウォームアップ
                div_factor=25.0,  # 初期学習率 = max_lr/div_factor
                final_div_factor=10000.0  # 最終学習率 = max_lr/final_div_factor
            )
            
            # モデルのトレーニング
            model_checkpoint_path = f'gcn_ms_model_{model_idx}.pth'
            
            train_losses, val_losses, val_cosine_similarities = train_model(
                model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device
            )
            
            # モデルのチェックポイントを保存
            model_checkpoints.append(f'best_gcn_ms_model_{model_idx}.pth')
            torch.save(model.state_dict(), model_checkpoints[-1])
            
            # 学習曲線をプロット
            plt.figure(figsize=(15, 5))
            
            plt.subplot(1, 2, 1)
            plt.plot(train_losses, label='Training Loss')
            plt.plot(val_losses, label='Validation Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.legend()
            plt.title(f'Loss Curves - Model {model_idx+1}')
            
            plt.subplot(1, 2, 2)
            plt.plot(val_cosine_similarities, label='Validation Cosine Similarity')
            plt.xlabel('Epoch')
            plt.ylabel('Cosine Similarity')
            plt.legend()
            plt.title(f'Cosine Similarity - Model {model_idx+1}')
            
            plt.tight_layout()
            plt.savefig(f'learning_curves_model_{model_idx}.png')
            plt.close()
            
        except Exception as e:
            print(f"モデル {model_idx+1} のトレーニング中にエラーが発生しました: {str(e)}")
            # エラーの詳細情報を表示
            import traceback
            traceback.print_exc()
    
    # アンサンブルモデルの作成
    if model_checkpoints:
        print("\nアンサンブルモデルを作成中...")
        ensemble = create_ensemble_from_checkpoints(
            HybridGNNModel,
            model_checkpoints,
            node_features,
            edge_features,  # +6を追加しない
            hidden_channels,
            MAX_MZ,
            device
        )
        
        # テストデータでの評価
        print("テストデータでアンサンブルモデルを評価中...")
        test_y_true = []
        test_y_pred = []
        
        with torch.no_grad():
            for graph_data, mass_spectrum, fragment_pattern in tqdm(test_loader, desc="Testing"):
                graph_data = graph_data.to(device)
                mass_spectrum = mass_spectrum.to(device)
                
                output, _ = ensemble(graph_data)
                
                test_y_true.append(mass_spectrum.cpu().numpy())
                test_y_pred.append(output.cpu().numpy())
        
        test_y_true_concat = np.concatenate(test_y_true)
        test_y_pred_concat = np.concatenate(test_y_pred)
        test_cosine_sim = cosine_similarity_score(test_y_true_concat, test_y_pred_concat)
        
        print(f"アンサンブルモデル - テストデータでのコサイン類似度: {test_cosine_sim:.4f}")
        
        # サンプルのマススペクトル予測結果を可視化（上位10サンプル）
        n_samples = min(10, len(test_y_true_concat))
        plt.figure(figsize=(20, 4 * n_samples))
        
        # 最も良いコサイン類似度のサンプルを検索
        sample_similarities = []
        for i in range(len(test_y_true_concat)):
            true_vec = test_y_true_concat[i]
            pred_vec = test_y_pred_concat[i]
            similarity = cosine_similarity(
                true_vec.reshape(1, -1), 
                pred_vec.reshape(1, -1)
            )[0][0]
            sample_similarities.append((i, similarity))
        
        # コサイン類似度の高い順にソート
        sample_similarities.sort(key=lambda x: x[1], reverse=True)
        
        # 上位サンプルの可視化
        for i in range(n_samples):
            idx, similarity = sample_similarities[i]
            true_spec = test_y_true_concat[idx]
            pred_spec = test_y_pred_concat[idx]
            
            plt.subplot(n_samples, 2, 2*i + 1)
            plt.stem(np.arange(len(true_spec)), true_spec, markerfmt=" ")
            plt.xlabel('m/z')
            plt.ylabel('Intensity')
            plt.title(f'True Spectrum - Sample {idx}')
            
            plt.subplot(n_samples, 2, 2*i + 2)
            plt.stem(np.arange(len(pred_spec)), pred_spec, markerfmt=" ", linefmt='r-')
            plt.xlabel('m/z')
            plt.ylabel('Intensity')
            plt.title(f'Predicted Spectrum - Cosine Sim: {similarity:.4f}')
        
        plt.tight_layout()
        plt.savefig('ensemble_spectrum_predictions.png')
        plt.show()
    else:
        print("モデルのトレーニングに失敗したため、アンサンブルを作成できませんでした。")
        
if __name__ == "__main__":
    main()
